{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e34995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:19:24.117748Z",
     "iopub.status.busy": "2025-06-19T12:19:24.117429Z",
     "iopub.status.idle": "2025-06-19T12:19:46.913181Z",
     "shell.execute_reply": "2025-06-19T12:19:46.912112Z"
    },
    "papermill": {
     "duration": 22.802681,
     "end_time": "2025-06-19T12:19:46.914819",
     "exception": false,
     "start_time": "2025-06-19T12:19:24.112138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:19:26.666648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750335566.937850      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750335567.021656      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39b2026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T12:19:46.924142Z",
     "iopub.status.busy": "2025-06-19T12:19:46.923532Z",
     "iopub.status.idle": "2025-06-19T12:19:47.271278Z",
     "shell.execute_reply": "2025-06-19T12:19:47.269919Z"
    },
    "papermill": {
     "duration": 0.353738,
     "end_time": "2025-06-19T12:19:47.272676",
     "exception": true,
     "start_time": "2025-06-19T12:19:46.918938",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/seminar-lstm/Gesamterzeugung_hourly.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/2746907108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/seminar-lstm/Gesamterzeugung_hourly.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mneu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Datum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üïê Typ der ersten Spalte: {type(neu.iloc[1, 0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50003\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/seminar-lstm/Gesamterzeugung_hourly.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_path = \"/kaggle/input/seminar-lstm/Gesamterzeugung_hourly.csv\"\n",
    "neu = pd.read_csv(csv_path, index_col='Datum', parse_dates=True)\n",
    "print(f\"üïê Typ der ersten Spalte: {type(neu.iloc[1, 0])}\")\n",
    "display(neu.iloc[50000:50003])\n",
    "neu.head(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfaed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T10:39:57.314415Z",
     "iopub.status.busy": "2025-06-06T10:39:57.313711Z",
     "iopub.status.idle": "2025-06-06T10:39:57.829493Z",
     "shell.execute_reply": "2025-06-06T10:39:57.828575Z",
     "shell.execute_reply.started": "2025-06-06T10:39:57.314383Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seq_length = 336 #(336)\n",
    "\n",
    "#Preprocessing, Normalisierung\n",
    "\n",
    "\n",
    "Strom_train = neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]\n",
    "mean = Strom_train.mean()\n",
    "std = Strom_train.std()\n",
    "\n",
    "#Dataset in Training Validation Testset einteilen\n",
    "\n",
    "Strom_train = (((neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_valid = (((neu[\"Stromerzeugung Gesamt\"][\"2018-07-01 00:00:00\":\"2018-12-31 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_test = (((neu[\"Stromerzeugung Gesamt\"][\"2019-01-01 00:00:00\":\"2019-06-01 23:00:00\"]) - mean)/std)\n",
    "\n",
    "\n",
    "#Training und Validation Set aufteilen in Targets & Inputs\n",
    "\n",
    "def split_inputs_and_targets(time_series, ahead=24): # target_col=1):\n",
    " return time_series[:, :-ahead], time_series[:, -ahead:]\n",
    "\n",
    "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    " Strom_train.to_numpy(),\n",
    " targets=None,\n",
    " sequence_length=seq_length + 24,\n",
    " batch_size=128,\n",
    " shuffle=False,\n",
    " seed=42\n",
    ").map(split_inputs_and_targets)\n",
    "\n",
    "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    " Strom_valid.to_numpy(),\n",
    " targets=None,\n",
    " sequence_length=seq_length + 24,\n",
    " batch_size=128\n",
    ").map(split_inputs_and_targets)\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_ds shape:\")\n",
    "inputs, targets = next(iter(train_ds))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "print(\"valid_ds shape:\")\n",
    "inputs, targets = next(iter(valid_ds))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df44fe5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-05T23:10:58.145270Z",
     "iopub.status.busy": "2025-06-05T23:10:58.144936Z",
     "iopub.status.idle": "2025-06-06T01:49:55.025829Z",
     "shell.execute_reply": "2025-06-06T01:49:55.024897Z",
     "shell.execute_reply.started": "2025-06-05T23:10:58.145247Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel)\n",
    "model_name = \"model_GRU32x2_gradclip_RedPl_seq336_B128\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.GRU(32, \n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, dropout=0.1),\n",
    "    tf.keras.layers.Dense(24)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd143e0",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-06T10:40:18.174681Z",
     "iopub.status.busy": "2025-06-06T10:40:18.173728Z",
     "iopub.status.idle": "2025-06-06T13:05:29.655663Z",
     "shell.execute_reply": "2025-06-06T13:05:29.654909Z",
     "shell.execute_reply.started": "2025-06-06T10:40:18.174654Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel)\n",
    "model_name = \"model_GRU32x2_gradclip_RedPl_seq336_B128\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.GRU(32, \n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, dropout=0.1),\n",
    "    tf.keras.layers.Dense(24)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36090ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:01:49.736539Z",
     "iopub.status.busy": "2025-06-06T09:01:49.735877Z",
     "iopub.status.idle": "2025-06-06T09:01:49.742390Z",
     "shell.execute_reply": "2025-06-06T09:01:49.741546Z",
     "shell.execute_reply.started": "2025-06-06T09:01:49.736513Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f26ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T13:25:34.194166Z",
     "iopub.status.busy": "2025-06-06T13:25:34.193794Z",
     "iopub.status.idle": "2025-06-06T13:25:34.597027Z",
     "shell.execute_reply": "2025-06-06T13:25:34.596097Z",
     "shell.execute_reply.started": "2025-06-06T13:25:34.194142Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seq_length = 336 #(336)\n",
    "\n",
    "#Preprocessing, Normalisierung\n",
    "\n",
    "\n",
    "Strom_train = neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]\n",
    "mean = Strom_train.mean()\n",
    "std = Strom_train.std()\n",
    "\n",
    "#Dataset in Training Validation Testset einteilen\n",
    "\n",
    "Strom_train = (((neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_valid = (((neu[\"Stromerzeugung Gesamt\"][\"2018-07-01 00:00:00\":\"2018-12-31 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_test = (((neu[\"Stromerzeugung Gesamt\"][\"2019-01-01 00:00:00\":\"2019-06-01 23:00:00\"]) - mean)/std)\n",
    "\n",
    "\n",
    "#Training und Validation Set aufteilen in Targets & Inputs\n",
    "\n",
    "def split_inputs_and_targets1(time_series, ahead=1): # target_col=1):\n",
    " return time_series[:, :-ahead], time_series[:, -ahead:]\n",
    "    \n",
    "train_ds1 = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    Strom_train.to_numpy(),\n",
    "    targets=None,\n",
    "    sequence_length=seq_length + 1,  \n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ").map(split_inputs_and_targets1)\n",
    "\n",
    "valid_ds1 = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    Strom_valid.to_numpy(),\n",
    "    targets=None,\n",
    "    sequence_length=seq_length + 1,\n",
    "    batch_size=128\n",
    ").map(split_inputs_and_targets1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_ds1 shape:\")\n",
    "inputs, targets = next(iter(train_ds1))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "print(\"valid_ds1 shape:\")\n",
    "inputs, targets = next(iter(valid_ds1))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb1583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T13:25:42.409769Z",
     "iopub.status.busy": "2025-06-06T13:25:42.409451Z",
     "iopub.status.idle": "2025-06-06T15:55:38.408683Z",
     "shell.execute_reply": "2025-06-06T15:55:38.407874Z",
     "shell.execute_reply.started": "2025-06-06T13:25:42.409749Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel)\n",
    "model_name = \"model_1Hour_GRU32x2_gradclip_RedPl_seq336_B128\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.GRU(32, \n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, dropout=0.1),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds1,\n",
    "    validation_data=valid_ds1,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caace93",
   "metadata": {
    "collapsed": true,
    "execution": {
     "execution_failed": "2025-06-06T20:29:47.602Z",
     "iopub.execute_input": "2025-06-06T15:55:38.410810Z",
     "iopub.status.busy": "2025-06-06T15:55:38.410538Z",
     "iopub.status.idle": "2025-06-06T18:25:02.736153Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel) \n",
    "model_name = \"model_1Hour_LSTM32x2_gradclip_RedPl_seq336_B128\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.LSTM(32, \n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, dropout=0.1),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds1,\n",
    "    validation_data=valid_ds1,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1e887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T09:04:42.115159Z",
     "iopub.status.busy": "2025-06-06T09:04:42.114543Z",
     "iopub.status.idle": "2025-06-06T09:04:42.407499Z",
     "shell.execute_reply": "2025-06-06T09:04:42.406742Z",
     "shell.execute_reply.started": "2025-06-06T09:04:42.115136Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seq_length = 336 #(336)\n",
    "\n",
    "#Preprocessing, Normalisierung\n",
    "\n",
    "\n",
    "Strom_train = neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]\n",
    "mean = Strom_train.mean()\n",
    "std = Strom_train.std()\n",
    "\n",
    "#Dataset in Training Validation Testset einteilen\n",
    "\n",
    "Strom_train = (((neu[\"Stromerzeugung Gesamt\"][\"2015-01-01 00:00:00\":\"2018-06-30 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_valid = (((neu[\"Stromerzeugung Gesamt\"][\"2018-07-01 00:00:00\":\"2018-12-31 23:00:00\"]) - mean)/std)\n",
    "\n",
    "Strom_test = (((neu[\"Stromerzeugung Gesamt\"][\"2019-01-01 00:00:00\":\"2019-06-01 23:00:00\"]) - mean)/std)\n",
    "\n",
    "\n",
    "#Training und Validation Set aufteilen in Targets & Inputs\n",
    "\n",
    "def split_inputs_and_targets1(time_series, ahead=1): # target_col=1):\n",
    " return time_series[:, :-ahead], time_series[:, -ahead:]\n",
    "    \n",
    "train_ds1 = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    Strom_train.to_numpy(),\n",
    "    targets=None,\n",
    "    sequence_length=seq_length + 1,  \n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ").map(split_inputs_and_targets1)\n",
    "\n",
    "valid_ds1 = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    Strom_valid.to_numpy(),\n",
    "    targets=None,\n",
    "    sequence_length=seq_length + 1,\n",
    "    batch_size=64\n",
    ").map(split_inputs_and_targets1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_ds1 shape:\")\n",
    "inputs, targets = next(iter(train_ds1))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n",
    "print(\"valid_ds1 shape:\")\n",
    "inputs, targets = next(iter(valid_ds1))\n",
    "print(\"Inputs shape:\", inputs.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53235714",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-06T09:04:50.599995Z",
     "iopub.status.busy": "2025-06-06T09:04:50.599402Z",
     "iopub.status.idle": "2025-06-06T09:16:29.736891Z",
     "shell.execute_reply": "2025-06-06T09:16:29.736128Z",
     "shell.execute_reply.started": "2025-06-06T09:04:50.599970Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel)\n",
    "model_name = \"model_1Hour_LSTM64_gradclip_RedPl_seq336\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),    \n",
    "    tf.keras.layers.LSTM(64, dropout=0.1),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds1,\n",
    "    validation_data=valid_ds1,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a609e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-06T09:16:48.744157Z",
     "iopub.status.busy": "2025-06-06T09:16:48.743624Z",
     "iopub.status.idle": "2025-06-06T09:28:44.367951Z",
     "shell.execute_reply": "2025-06-06T09:28:44.367331Z",
     "shell.execute_reply.started": "2025-06-06T09:16:48.744135Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from IPython.display import FileLink  # F√ºr Kaggle-Downloads\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "# Modellname und Basis-Speicherpfad (Kaggle-kompatibel)\n",
    "model_name = \"model_1Hour_GRU64_gradclip_RedPl_seq336\"\n",
    "base_model_dir = \"./models/\"  # üÜï Lokaler Pfad in Kaggle\n",
    "\n",
    "\n",
    "# Unterordner f√ºr das Modell erstellen\n",
    "model_dir = os.path.join(base_model_dir, model_name)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Pfade im Unterordner\n",
    "model_path = os.path.join(model_dir, f\"{model_name}-epoch-{{epoch:02d}}.keras\")\n",
    "history_path = os.path.join(model_dir, f\"{model_name}_history.pkl\")\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, 1)),    \n",
    "    tf.keras.layers.GRU(64, dropout=0.1),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.005,\n",
    "    clipvalue=1.0\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_best_only=False,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds1,\n",
    "    validation_data=valid_ds1,\n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint_cb, lr_scheduler]\n",
    ")\n",
    "\n",
    "# History speichern\n",
    "with open(history_path, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "print(f\"\\nGespeicherte Dateien in Kaggle:\")\n",
    "print(f\"- Modell-Checkpoints: {model_dir}/*.keras\")\n",
    "print(f\"- Trainingshistorie: {history_path}\")\n",
    "\n",
    "#Download-Links f√ºr Kaggle\n",
    "# Erstelle ZIP f√ºr einfachen Download\n",
    "import shutil\n",
    "shutil.make_archive(model_name, 'zip', model_dir)\n",
    "display(FileLink(f\"{model_name}.zip\"))  # Download-Link f√ºr gesamtes Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907d7ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7598897,
     "sourceId": 12071853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.363946,
   "end_time": "2025-06-19T12:19:50.074842",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-19T12:19:18.710896",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
